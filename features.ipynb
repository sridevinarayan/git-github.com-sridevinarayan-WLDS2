{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concise feature extraction\n",
    "import librosa\n",
    "from librosa import feature\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from functools import reduce\n",
    "\n",
    "def get_percussion(y):\n",
    "    har, perc =   librosa.decompose.hpss(np.abs(librosa.stft(y)), margin=16)\n",
    "    return perc\n",
    "\n",
    "\n",
    "fn_list_i = [\n",
    "   # feature.chroma_stft,\n",
    "    feature.spectral_centroid,\n",
    "    feature.spectral_bandwidth,\n",
    "    feature.spectral_rolloff, \n",
    "    librosa.onset.onset_strength,\n",
    "    librosa.feature.mfcc\n",
    "]\n",
    "\n",
    "fn_list_ii = [\n",
    "#    feature.rmse,\n",
    "    feature.zero_crossing_rate,\n",
    "    get_percussion\n",
    "]\n",
    "\n",
    "\n",
    "def get_feature_vector(y, sr):\n",
    "   feat_vect_i = [np.mean(funct(y, sr)) for funct in fn_list_i]\n",
    "   feat_vect_ii = [np.mean(funct(y)) for funct in fn_list_ii]\n",
    "   feature_vector = feat_vect_i + feat_vect_ii\n",
    "   return feature_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classify features\n",
    "# The naming convention for files:\n",
    "#Anything with *D.wav is a a water leak - ht, medium, slow, fast drips\n",
    "#Anything with *N.wav is white  noise / ambient nouse\n",
    "#Anything with *R?.wav or HW is running water or running tap or falling from a height\n",
    "\n",
    "run = ['RW', 'RT', 'HW']\n",
    "leak = ['FD', 'MD', 'SD', 'SP', 'HD']\n",
    "noise = ['WN', 'AN']\n",
    "\n",
    "def classify_datafile(datafile): \n",
    "    file, ext = os.path.splitext(datafile)\n",
    "    [rest, cl_str] = file.rsplit(\"_\", 1)\n",
    "    #print(datafile)\n",
    "    if cl_str in leak: \n",
    "        classify =  ['leak']\n",
    "    # elif cl_str in run :\n",
    "    #     classify =  ['running water']\n",
    "    else:\n",
    "        classify = ['running water/noise']\n",
    "    #endif\n",
    "    #print(classify)\n",
    "    return classify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "#main cell \n",
    "#reads each file, extracts features\n",
    "#and saves in a ''/Users/ns/development/iisc/WLDS/data/out/dataset_1.csv'\n",
    "#this dataset should be used for training  \n",
    "###---------------------------------------------\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# read each file\n",
    "# extract features in a dict\n",
    "# put them in a list\n",
    "# convert to data frame\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data_file_path = '/Users/ns/development/iisc/WLDS2/data/extra/'\n",
    "out_file_path = '/Users/ns/development/iisc/WLDS2/data/extra/'\n",
    "header = ['Sample#', 'spectral_centroid', 'spectral_bandwidth',\n",
    "          'spectral_rolloff', 'onset_strength',\n",
    "          'mfcc', 'zero_crossing_rate', 'percussion', 'label']\n",
    "f_header = header[1:7] #extract only features\n",
    "samples = os.listdir(data_file_path)\n",
    "#print(samples)\n",
    "feature_list = []\n",
    "for datafile in tqdm(samples):\n",
    "    data_f = data_file_path + datafile\n",
    "    print('Extracting Features for  ',data_f)\n",
    "    y, sr = librosa.load(data_f)\n",
    "    feature_vector = get_feature_vector(y, sr) \n",
    "    label = classify_datafile(datafile)\n",
    "    Sno = samples.index(datafile) \n",
    "    feature_list.append([Sno] + feature_vector + label)\n",
    "#end for\n",
    "\n",
    "#normalize and save the dataset. \n",
    "\n",
    "#print(feature_list) \n",
    "feature_df = pd.DataFrame(feature_list, columns=header)\n",
    "#feature_df[f_header] = normalize(feature_df[f_header].to_numpy())\n",
    "feature_df.to_csv(out_file_path + 'dataset_1.csv', index=False)\n",
    "\n",
    "##end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    " Copyright 2022 ns\n",
    " \n",
    " Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    " you may not use this file except in compliance with the License.\n",
    " You may obtain a copy of the License at\n",
    " \n",
    "     http://www.apache.org/licenses/LICENSE-2.0\n",
    " \n",
    " Unless required by applicable law or agreed to in writing, software\n",
    " distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    " WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    " See the License for the specific language governing permissions and\n",
    " limitations under the License.\n",
    "-->\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
